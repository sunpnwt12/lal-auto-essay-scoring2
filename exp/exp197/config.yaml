apex: true
batch_size: 8
ccp_pooling_last: 4
clip_grad_norm: true
criterion: mse
cv_strat: multilabel_stratified_kfold
data_path: /kaggle/input/learning-agency-lab-automated-essay-scoring-2/
decoder_lr: 1.0e-05
encoder_lr: 1.0e-05
exp: '197'
extra_data_path: /kaggle/input/lal-aes2-create-prompt-data/cleaned_persaude.csv
freeze_embeddings: false
freeze_n_layers: 0
gradient_checkpointing: true
layerwise_lr_decay: 0.9
lstm_hidden: 768
max_grad_norm: 1.0
max_len: 1024
model_name: microsoft/deberta-base
multi_dropout: false
num_epochs: 3
num_folds: 4
num_labels: 1
optim_betas1: 0.9
optim_betas2: 0.999
optim_eps: 1.0e-06
optimizer: adamw
poly_power: 0.05
pooling_layer: attention_pooling
save_path: /kaggle/working/
scheduler: linear_warmup
seed: 42
target_col: label
train_col: tokened_text
train_fold_list:
- 0
- 1
- 2
- 3
verbose_step: 200
warmup_ratios: 0.0
weight_decay: 0.01
wlp_layer_start: 4
